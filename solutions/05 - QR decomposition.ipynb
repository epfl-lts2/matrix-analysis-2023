{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4b696ef-2ecd-45e0-8a9f-d8103b517496",
   "metadata": {},
   "source": [
    "# Matrix Analysis 2023 - EE312\n",
    "\n",
    "## Week 5 - QR DECOMPOSITION\n",
    "[LTS2](https://lts2.epfl.ch)\n",
    "\n",
    "### Objectives\n",
    "The goal of this week's exercise session is to study some aspects of the QR decomposition and its connections to projections and inverse.\n",
    "\n",
    "Let us consider a square $n\\times n$ real matrix $A$ having linearly independent columns (NB: QR factorization is applicable to rectangular matrices, we consider square matrices for simplification). The QR decomposition of $A$ is $A=QR$ where $Q$ is an orthogonal matrix and $R$ and upper-triangular matrix.\n",
    "\n",
    "We will study two methods that can be used to compute the QR decomposition of a matrix and an application.\n",
    "\n",
    "# 1. Gram-Schmidt orthogonalization\n",
    "\n",
    "Reminder:\n",
    "- The projection operator of a vector $v$ on another $u$ is given by $P_u v = \\frac{\\langle u,v \\rangle}{\\langle u,u \\rangle}u$.\n",
    "- Gram-Schmidt orthogonalization process of a basis $(v_1, v_2, ..., v_{n})$ produces an orthormal basis $(e_1, e_2, ..., e_{n})$ as follows:\n",
    "\n",
    "$u_1 = v_1$, $e_1 = \\frac{u_1}{\\|u_1\\|}$\n",
    "\n",
    "$u_2 = v_2 - P_{u_1}v_2$, $e_2 = \\frac{u_2}{\\|u_2\\|}$\n",
    "\n",
    "$u_3 = v_3 - P_{u_2}v_3 - P_{u_1}v_3$, $e_3 = \\frac{u_3}{\\|u_3\\|}$\n",
    "\n",
    "...\n",
    "\n",
    "$u_k = v_k - \\sum_{j=1}^{k-1} P_{u_j}v_{k}$, $e_k = \\frac{u_k}{\\|u_k\\|}$\n",
    "\n",
    "\n",
    "\n",
    "## 1.1 \n",
    "We perform the QR decomposition of $A$ by applying Gram-Schmidt to its column vectors of $A$ (denoted by $a_k, k=1,...n)$. $Q$ is made of the column vectors $e_k$, i.e. $Q=(e_1|e_2|...|e_{n})$. Let us denote by $r_{jk}$ the coefficients of $R$.\n",
    "\n",
    "Prove that $r_{kk} = \\|u_k\\|$, $r_{jk} = <e_j, a_k>$ for $j<k$ and $r_{jk} = 0$ for $j>k$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e301db37-b940-4da2-a2f4-4ce603a881e1",
   "metadata": {},
   "source": [
    "*Answer*: Using the Gram-Schmidt formula, we can write:\n",
    "$a_k = u_k + \\sum_{j=1}^{k-1} P_{u_j}a_{k} = u_k + \\sum_{j=1}^{k-1}\\frac{<u_j, a_k>}{\\|u_j\\|^2}u_j = \\|u_k\\|e_k + \\sum_{j=1}^{k-1}\\frac{<u_j, a_k>}{\\|u_j\\|^2}\\|u_j\\|e_j = \\|u_k\\|e_k + \\sum_{j=1}^{k-1}\\frac{<u_j, a_k>}{\\|u_j\\|}e_j = \\|u_k\\|e_k + \\sum_{j=1}^{k-1}<e_j, a_k>e_j $\n",
    "\n",
    "Using the definition of $Q$ and $R$ we also have $a_k = \\sum_{j=1}^n r_{jk} e_j$. Using the equality above, this proves that $r_{kk} = \\|u_k\\|$, $r_{jk} = <e_j, a_k>$ for $j<k$ and $r_{jk} = 0$ for $j>k$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93abc7b-3142-4515-a03e-a7b582af3815",
   "metadata": {},
   "source": [
    "## 1.2\n",
    "Implement a function that performs the QR decomposition of a square $n\\times n$ matrix using the Gram-Schmidt orthogonalization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c455d1c2-2f2c-4316-81e5-7eb202f87946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def qr_decomp_gs(A):\n",
    "    n = A.shape[0]\n",
    "    Q = np.zeros((n,n))\n",
    "    R = np.zeros((n,n))\n",
    "    U = np.zeros((n,n))\n",
    "    U[:, 0] = A[:, 0]\n",
    "    Q[:, 0] = A[:, 0]/np.linalg.norm(A[:, 0])\n",
    "    R[0, 0] = np.linalg.norm(A[:, 0])\n",
    "    for k in np.arange(1, n):\n",
    "        acc = np.zeros((n))\n",
    "        for j in np.arange(0, k):\n",
    "            R[j, k] = np.dot(A[:, k], Q[:, j])\n",
    "            acc = acc + (np.dot(A[:, k], U[:, j])/np.dot(U[:, j], U[:, j]))*U[:, j]\n",
    "\n",
    "        U[:, k] = A[:, k] - acc\n",
    "        Q[:, k] = U[:, k]/np.linalg.norm(U[:, k])\n",
    "        R[k, k] = np.linalg.norm(U[:, k])\n",
    "            \n",
    "    return Q,R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3460934-a3b6-4c79-bd61-f1eeac645f73",
   "metadata": {},
   "source": [
    "## 1.3\n",
    "Using the properties of Q, write a routine that can solve a linear system $Ax=b$ using the QR factorization of $A$ ($A$ being a $n\\times n$ real matrix).\n",
    "What property do the coefficients of $R$ need to satisfy for the solution to exist ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d826d398-835e-4dc4-82a8-5fbe0a0fcc40",
   "metadata": {},
   "source": [
    "*Answer:* We have $Ax = b$ and $A=QR$ with $Q$ orthogonal hence $Q^TQRx = Q^Tb$ i.e. $Rx=Q^Tb$. \n",
    "\n",
    "Given $R$ is upper triangular, we do not need to perform compute $R^{-1}$ explicitely and the system can be solved using backsubstitution. \n",
    "Since we divide by $r_{kk}$ to get the $x_k$ values, those diagonal coefficients have to be non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00940be7-8e7b-42f9-a691-ea4631a235e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(A, b):\n",
    "    n = A.shape[0]\n",
    "    Q, R = qr_decomp_gs(A)\n",
    "    c = Q.T@b\n",
    "    x = np.zeros(n)\n",
    "    x[n-1] = c[n-1]/R[n-1, n-1]\n",
    "    for k in np.arange(n-2, -1, -1):\n",
    "        s = np.dot(R[k, k+1:], x[k+1:])\n",
    "        x[k] = (c[k] - s)/R[k,k]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ed57911-5f2e-4953-b2ce-26dd669daa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small numerical example\n",
    "A=np.array([[12,-51,4],[6,167,-68],[-4,24,-41]])\n",
    "b=np.array([1, -1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fd4a98d-cdc8-4c09-914c-0e95fc0483ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = solve(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "797892aa-a084-4524-8f74-545094e5bdcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., -1.,  1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A@x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2d1ec0-f35b-4ff0-9b69-feeb56a7bac0",
   "metadata": {},
   "source": [
    "## 1.4\n",
    "Consider the matrix \n",
    "$B=\\begin{pmatrix}1 & 1 & 1 \\\\ \\varepsilon & 0 & 0\\\\ 0 & \\varepsilon & 0\\end{pmatrix}$, with $\\varepsilon$ being small enough (typically $10^{-8}$ or smaller). In that case, is the method used to compute Q and R still reliable (and why) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e72d9825-52ca-426c-a444-63c414f9b6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps=1e-8\n",
    "B=np.array([[1,1,1],[eps,0,0],[0,eps,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e263494f-c307-4243-a6f4-bcecc88dd7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.e+00, 1.e+00, 1.e+00],\n",
       "       [1.e-08, 0.e+00, 0.e+00],\n",
       "       [0.e+00, 1.e-08, 0.e+00]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ae71b30b-fee6-4739-9fa1-480c55189a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "QB,RB = qr_decomp_gs(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e4751e5c-1648-44e3-bed0-f0c49d284a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.0e+00,  1.0e-08,  0.0e+00],\n",
       "       [ 1.0e-08,  1.5e+00, -5.0e-01],\n",
       "       [ 0.0e+00, -5.0e-01,  5.0e-01]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QB@QB.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "00c65b35-3c8c-4ea6-a7ee-dd188a08aa40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QB@RB-B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c89cbf-3358-4409-9803-34f85fe4f00d",
   "metadata": {},
   "source": [
    "We still have $QR=B$, but $Q$ is no longer orthogonal ! This is due to the numerical precision of the computer, that rounds off the norm of the first column of $B$ to 1, when it should be $\\sqrt{1+\\varepsilon^2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0911e0f-6edc-4bbd-b5c7-b82c2257151c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(B[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c4e544-a4fd-4e5a-a26c-5f6cb66686d4",
   "metadata": {},
   "source": [
    "# 2. Householder reflections\n",
    "\n",
    "We will now study and implement another method to perform the QR decomposition that is more resistant to the numerical issues mentioned above.\n",
    "\n",
    "## 2.1 \n",
    "Let $v$ be a unit vector and the associated transform $H_v=I-2vv^T$. Prove that $H_v$ is orthogonal and that it preserves the norm. What is the effect of $H_v$ on a vector $u$ orthogonal to $v$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7f7184-32f2-480e-adec-877ce846d161",
   "metadata": {},
   "source": [
    "*Answer:* $H_vH_v^T = (I - 2vv^T)(I - 2vv^T)^T = (I - 2vv^T)(I - 2vv^T) = I -4vv^T + 4v(v^Tv)v^T$\n",
    "\n",
    "Since $v$ is a unit vector, $v^Tv=1$ i.e. $H_vH_v^T = I - 4vv^T + 4vv^T = I$.\n",
    "\n",
    "$\\|H_vu\\|^2 = (H_vu)^TH_vu = u^TH_v^TH_vu =  u^Tu = \\|u\\|^2$, $H_v$, as any orthogonal transform preserves norm. \n",
    "\n",
    "If $u$ is orthogonal to $v$, we have $v^Tu=0$.\n",
    "$H_vu = u - 2 vv^Tu = u$. $H_v$ does not modify vectors orthogonal to $v$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5352bba-8353-443b-aa53-249b0341fdc2",
   "metadata": {},
   "source": [
    "## 2.2 \n",
    "What is the effect of $H_v$ on any vector $u$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57509402-a225-478b-a41c-966ab2eabec5",
   "metadata": {},
   "source": [
    "*Answer:* $H_vu = u - 2vv^Tu = u - 2<v,u>v$\n",
    "\n",
    "The term $<v,u>v$ is in fact the orthogonal projection of $u$ on $v$ (remember $<v,v>=\\|v\\|=1)$. If we draw the effect of $H_v$ on $u$, it yields the reflection of $u$ around $v^\\perp$.\n",
    "\n",
    "![householder](images/householder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b255fc-51a6-4f49-ab27-e00ea87e8394",
   "metadata": {},
   "source": [
    "## 2.3\n",
    "The goal is now to design $n$ reflection operators $H_{v_1}, H_{v_2}, ..., H_{v_n}$ s.t. we have\n",
    "$$\n",
    "H_{v_n}...H_{v_2}H_{v_1}A = R,\n",
    "$$\n",
    "where $R$ is an upper triangular matrix. \n",
    "\n",
    "The reflection operations are meant to be \"progressive\", i.e. $H_{v_k}...H_{v_1}A$ has its $k$ first columns upper triangular.\n",
    "\n",
    "- Find $v_1$ and $\\alpha$ s.t. $H_{v_1}a_1 = \\alpha e_1$, where $a_1$ is the first column of $A$ and $e_1 = \\begin{pmatrix}1\\\\0\\\\ \\vdots \\\\0 \\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8943fad5-f53f-46c2-8222-ece12e92e8ae",
   "metadata": {},
   "source": [
    "*Answer:* $H_{v_1}a_1 = (I -2v_1v_1^T)a_1 = \\alpha e_1$, i.e. $a_1 - 2v_1v_1^Ta_1 = \\alpha e_1$, \n",
    "and finally $a_1 - \\alpha e_1 = 2 <v_1, a_1> v_1$, which implies that $v_1$ is colinear to $a_1 - \\alpha e_1$.\n",
    "Remembering that $H_{v_1}$ preserves norm, we have $\\|H_{v_1}a_1\\| = \\|a_1\\| = |\\alpha|$.\n",
    "Therefore $v_1 = \\frac{1}{\\|a_1 \\pm \\|a_1\\|e_1\\|}(a_1 \\pm \\|a_1\\|e_1)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e465b25-b22f-42ce-9b47-ee357e5f788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "u=A[:,0] - np.linalg.norm(A[:,0])*np.array([1, 0, 0])\n",
    "v = u/np.linalg.norm(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7de513f7-c880-4158-8233-4c007e4baced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(A[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99549ef9-3fe3-4b89-adf7-c0eccbe7e1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "H=np.eye(3)- 2*np.kron(v, v).reshape((3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f176715-11d2-4e08-bf9f-129285864608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.4000000e+01, -4.4408921e-16,  8.8817842e-16])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H@A[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316e5ba6-ab4f-42f8-b4ac-5d0ddebef1d1",
   "metadata": {},
   "source": [
    "- Let us now generalize the previous step to find $H_{v_k}$. Given a vector $x = \\begin{pmatrix}x^U \\\\ x^L\\end{pmatrix}$ with $x^U \\in\\mathbb{R}^k$\n",
    "and $x^L\\in\\mathbb{R}^{n-k}$. \n",
    "\n",
    "Find a vector $v_k$ s.t. $H_{v_k}x = \\begin{pmatrix}x^U \\\\ \\alpha e_1^L \\end{pmatrix}$, where $\\alpha\\in\\mathbb{R}$ and \n",
    "$e_1^L = \\begin{pmatrix}1\\\\0\\\\ \\vdots \\\\0 \\end{pmatrix} \\in \\mathbb{R}^{n-k}$. \n",
    "\n",
    "It might be of help to write $v_k=\\begin{pmatrix}v_k^U\\\\v_k^L\\end{pmatrix}$, with $v_k^U \\in\\mathbb{R}^k$\n",
    "and $v_k^L\\in\\mathbb{R}^{n-k}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f784608-692e-49f4-9ba2-46ba114f53e5",
   "metadata": {},
   "source": [
    "*Answer:*  Since we have $H_{v_k} = I - 2v_kv_k^T = I - 2\\begin{pmatrix}v_k^U\\\\v_k^L\\end{pmatrix}\\begin{pmatrix}(v_k^U)^T & (v_k^L)^T\\end{pmatrix}$, which leads to $H_{v_k} = I - 2\\begin{pmatrix}v_k^U(v_k^U)^T & v_k^U(v_k^L)^T \\\\ (v_k^U)^Tv_k^L & v_k^L(v_k^L)^T\\end{pmatrix}$\n",
    "\n",
    "$H_{v_k}x = \\begin{pmatrix}x^U \\\\ x^L\\end{pmatrix} - 2\\begin{pmatrix}v_k^U(v_k^U)^T & v_k^U(v_k^L)^T \\\\ (v_k^U)^Tv_k^L & v_k^L(v_k^L)^T\\end{pmatrix}\\begin{pmatrix}x^U \\\\ x^L\\end{pmatrix}$\n",
    "\n",
    "If we want the upper part of $x$ to be preserved, it is relatively obvious that $v_k^U = 0$ will be needed, which leads to\n",
    "$H_{v_k}x = \\begin{pmatrix}x^U \\\\ x^L\\end{pmatrix} - 2\\begin{pmatrix}0 & 0 \\\\ 0 & v_k^L(v_k^L)^T\\end{pmatrix}\\begin{pmatrix}x^U \\\\ x^L\\end{pmatrix}$.\n",
    "\n",
    "The lower part yields: $x^L - 2 v_k^L(v_k^L)^Tx^L = x^L - 2<v_k^L, x^L>v_k^L  = \\alpha e_1^L$. As for $v_1$, we see that $v_k$ is colinear to $x^L - \\alpha e_1^L$, and the norm preservation property yields $\\alpha = \\pm\\|x^L\\|$.\n",
    "\n",
    "Finally, $v_k^L = \\frac{1}{\\|x^L \\pm \\|x^L\\|e_1^L \\|}(x^L \\pm \\|x^L\\|e_1^L)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497c386e-e498-4b85-ac8a-f3bea7230c6e",
   "metadata": {},
   "source": [
    "## 2.4\n",
    "We now have all we need to compute the $H_{v_k}$ matrices. Implement the QR decomposition using the above results. \n",
    "You should have found that the sign of $\\alpha$ can be either positive or negative. Choose $\\alpha$ to have the sign opposite to the one of $x^L_k$. (Use the `numpy.sign`function).\n",
    "\n",
    "Try your implementation on the $B$ matrix seen in ex. 1. What is the advantage of this implementation ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc13bdb8-5046-42e4-a3e0-f58654acce81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qr_decomp_hh(A):\n",
    "    n = A.shape[0]\n",
    "    E = np.eye(n)\n",
    "    Q = np.eye(n)\n",
    "    R = A.copy()\n",
    "    for k in np.arange(0, n):\n",
    "        u = np.zeros(n)\n",
    "        s = np.sign(R[k,k])\n",
    "        u[k:] = R[k:,k] + s*np.linalg.norm(R[k:,k])*E[k:, k]\n",
    "        v = u/np.linalg.norm(u)\n",
    "        H = E - 2*np.outer(v, v)\n",
    "        R = H@R\n",
    "        Q = H@Q\n",
    "    return Q.T,R # in this case we need the 'inverse' of Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f431c937-5870-467d-846e-8d3cffb9d7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qh, rh = qr_decomp_hh(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36c88ba6-ee63-4e2e-918f-8ea0cbfa2bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.55271368e-15,  4.26325641e-14, -1.59872116e-14],\n",
       "       [ 0.00000000e+00, -8.52651283e-14,  4.26325641e-14],\n",
       "       [ 4.44089210e-16, -3.55271368e-15,  0.00000000e+00]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qh@rh -A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "436af62c-bb45-4aef-9ac4-26a2fc1c8242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00,  1.26634814e-16,  1.11022302e-16],\n",
       "       [ 1.26634814e-16,  1.00000000e+00, -4.85722573e-17],\n",
       "       [ 1.11022302e-16, -4.85722573e-17,  1.00000000e+00]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qh@qh.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ffc670d-b80e-4ad0-bf4a-54ec5bc9aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qh, rh = qr_decomp_hh(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba84bb83-f5b0-4dce-88fa-a1237e01e453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00,  1.15805286e-23, -4.96308368e-24],\n",
       "       [ 1.15805286e-23,  1.00000000e+00,  4.44089210e-16],\n",
       "       [-4.96308368e-24,  4.44089210e-16,  1.00000000e+00]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qh@qh.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "111e727a-34a1-4cbd-9183-19806b3af7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  1.45050537e-23,  1.15805286e-23],\n",
       "       [ 0.00000000e+00, -4.96308368e-24, -4.96308368e-24]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qh@rh-B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514d869f-f2c7-40eb-b204-d593b6ee6133",
   "metadata": {},
   "source": [
    "The QR factorization using the Householder reflections is now giving correct results on $B$. This seems a better choice in terms of numerical stability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a56a243-e40a-47f3-bb9a-d64ed996a69a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
