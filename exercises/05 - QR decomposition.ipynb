{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4b696ef-2ecd-45e0-8a9f-d8103b517496",
   "metadata": {},
   "source": [
    "# Matrix Analysis 2023 - EE312\n",
    "\n",
    "## Week 5 - QR DECOMPOSITION\n",
    "[LTS2](https://lts2.epfl.ch)\n",
    "\n",
    "### Objectives\n",
    "The goal of this week's exercise session is to study some aspects of the QR decomposition and its connections to projections and inverse. We will study two methods that can be used to compute the QR decomposition of a matrix and an application.\n",
    "\n",
    "Please submit your answers individually.\n",
    "\n",
    "Let us consider a square $n\\times n$ real matrix $A$ having linearly independent columns (NB: QR factorization is applicable to rectangular matrices, we consider square matrices for simplification). The QR decomposition of $A$ is $A=QR$ where $Q$ is an orthogonal matrix and $R$ and upper-triangular matrix.\n",
    "\n",
    "\n",
    "\n",
    "# 1. Gram-Schmidt orthogonalization\n",
    "\n",
    "Reminder:\n",
    "- The projection operator of a vector $v$ on another $u$ is given by $P_u v = \\frac{\\langle u,v \\rangle}{\\langle u,u \\rangle}u$.\n",
    "- Gram-Schmidt orthogonalization process of a basis $(v_1, v_2, ..., v_{n})$ produces an orthormal basis $(e_1, e_2, ..., e_{n})$ as follows:\n",
    "\n",
    "$u_1 = v_1$, $e_1 = \\frac{u_1}{\\|u_1\\|}$\n",
    "\n",
    "$u_2 = v_2 - P_{u_1}v_2$, $e_2 = \\frac{u_2}{\\|u_2\\|}$\n",
    "\n",
    "$u_3 = v_3 - P_{u_2}v_3 - P_{u_1}v_3$, $e_3 = \\frac{u_3}{\\|u_3\\|}$\n",
    "\n",
    "...\n",
    "\n",
    "$u_k = v_k - \\sum_{j=1}^{k-1} P_{u_j}v_{k}$, $e_k = \\frac{u_k}{\\|u_k\\|}$\n",
    "\n",
    "\n",
    "\n",
    "## 1.1 \n",
    "We perform the QR decomposition of $A$ by applying Gram-Schmidt to its column vectors of $A$ (denoted by $a_k, k=1,...n)$. $Q$ is made of the column vectors $e_k$, i.e. $Q=(e_1|e_2|...|e_{n})$. Let us denote by $r_{jk}$ the coefficients of $R$.\n",
    "\n",
    "Prove that $r_{kk} = \\|u_k\\|$, $r_{jk} = <e_j, a_k>$ for $j<k$ and $r_{jk} = 0$ for $j>k$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e301db37-b940-4da2-a2f4-4ce603a881e1",
   "metadata": {},
   "source": [
    "*Answer:* \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93abc7b-3142-4515-a03e-a7b582af3815",
   "metadata": {},
   "source": [
    "## 1.2\n",
    "Implement a function that performs the QR decomposition of a square $n\\times n$ matrix using the Gram-Schmidt orthogonalization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c455d1c2-2f2c-4316-81e5-7eb202f87946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def qr_decomp_gs(A):\n",
    "    n = A.shape[0]\n",
    "    Q = np.zeros((n,n))\n",
    "    R = np.zeros((n,n))\n",
    "    # your code here\n",
    "    return Q,R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3460934-a3b6-4c79-bd61-f1eeac645f73",
   "metadata": {},
   "source": [
    "## 1.3\n",
    "Using the properties of Q, write a routine that can solve a linear system $Ax=b$ using the QR factorization of $A$ ($A$ being a $n\\times n$ real matrix).\n",
    "What property do the coefficients of $R$ need to satisfy for the solution to exist ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d826d398-835e-4dc4-82a8-5fbe0a0fcc40",
   "metadata": {},
   "source": [
    "*Answer:* \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00940be7-8e7b-42f9-a691-ea4631a235e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(A, b):\n",
    "    n = A.shape[0]\n",
    "    x = np.zeros(n)\n",
    "    # your code here\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed57911-5f2e-4953-b2ce-26dd669daa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small numerical example\n",
    "A=np.array([[12,-51,4],[6,167,-68],[-4,24,-41]])\n",
    "b=np.array([1, -1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd4a98d-cdc8-4c09-914c-0e95fc0483ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = solve(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797892aa-a084-4524-8f74-545094e5bdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "A@x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2d1ec0-f35b-4ff0-9b69-feeb56a7bac0",
   "metadata": {},
   "source": [
    "## 1.4\n",
    "Consider the matrix \n",
    "$B=\\begin{pmatrix}1 & 1 & 1 \\\\ \\varepsilon & 0 & 0\\\\ 0 & \\varepsilon & 0\\end{pmatrix}$, with $\\varepsilon$ being small enough (typically $10^{-8}$ or smaller). In that case, is the method used to compute Q and R still reliable (and why) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72d9825-52ca-426c-a444-63c414f9b6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps=1e-8\n",
    "B=# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e263494f-c307-4243-a6f4-bcecc88dd7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae71b30b-fee6-4739-9fa1-480c55189a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "QB,RB = qr_decomp_gs(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c65b35-3c8c-4ea6-a7ee-dd188a08aa40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24c89cbf-3358-4409-9803-34f85fe4f00d",
   "metadata": {},
   "source": [
    "*Answer:* \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c4e544-a4fd-4e5a-a26c-5f6cb66686d4",
   "metadata": {},
   "source": [
    "# 2. Householder reflections\n",
    "\n",
    "We will now study and implement another method to perform the QR decomposition that is more resistant to the numerical issues mentioned above.\n",
    "\n",
    "## 2.1 \n",
    "Let $v$ be a unit vector and the associated transform $H_v=I-2vv^T$. Prove that $H_v$ is orthogonal and that it preserves the norm. What is the effect of $H_v$ on a vector $u$ orthogonal to $v$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7f7184-32f2-480e-adec-877ce846d161",
   "metadata": {},
   "source": [
    "*Answer:* \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5352bba-8353-443b-aa53-249b0341fdc2",
   "metadata": {},
   "source": [
    "## 2.2 \n",
    "What is the effect of $H_v$ on any vector $u$ (drawing the planar case can be of help)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57509402-a225-478b-a41c-966ab2eabec5",
   "metadata": {},
   "source": [
    "*Answer:* \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b255fc-51a6-4f49-ab27-e00ea87e8394",
   "metadata": {},
   "source": [
    "## 2.3\n",
    "The goal is now to design $n$ reflection operators $H_{v_1}, H_{v_2}, ..., H_{v_n}$ s.t. we have\n",
    "$$\n",
    "H_{v_n}...H_{v_2}H_{v_1}A = R,\n",
    "$$\n",
    "where $R$ is an upper triangular matrix. \n",
    "\n",
    "The reflection operations are meant to be \"progressive\", i.e. $H_{v_k}...H_{v_1}A$ has its $k$ first columns upper triangular.\n",
    "\n",
    "- Find $v_1$ and $\\alpha$ s.t. $H_{v_1}a_1 = \\alpha e_1$, where $a_1$ is the first column of $A$ and $e_1 = \\begin{pmatrix}1\\\\0\\\\ \\vdots \\\\0 \\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8943fad5-f53f-46c2-8222-ece12e92e8ae",
   "metadata": {},
   "source": [
    "*Answer:* \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316e5ba6-ab4f-42f8-b4ac-5d0ddebef1d1",
   "metadata": {},
   "source": [
    "- Let us now generalize the previous step to find $H_{v_k}$. Given a vector $x = \\begin{pmatrix}x^U \\\\ x^L\\end{pmatrix}$ with $x^U \\in\\mathbb{R}^k$\n",
    "and $x^L\\in\\mathbb{R}^{n-k}$. \n",
    "\n",
    "Find a vector $v_k$ s.t. $H_{v_k}x = \\begin{pmatrix}x^U \\\\ \\alpha e_1^L \\end{pmatrix}$, where $\\alpha\\in\\mathbb{R}$ and \n",
    "$e_1^L = \\begin{pmatrix}1\\\\0\\\\ \\vdots \\\\0 \\end{pmatrix} \\in \\mathbb{R}^{n-k}$. \n",
    "\n",
    "It might be of help to write $v_k=\\begin{pmatrix}v_k^U\\\\v_k^L\\end{pmatrix}$, with $v_k^U \\in\\mathbb{R}^k$\n",
    "and $v_k^L\\in\\mathbb{R}^{n-k}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f784608-692e-49f4-9ba2-46ba114f53e5",
   "metadata": {},
   "source": [
    "*Answer:*  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497c386e-e498-4b85-ac8a-f3bea7230c6e",
   "metadata": {},
   "source": [
    "## 2.4\n",
    "We now have all we need to compute the $H_{v_k}$ matrices. Implement the QR decomposition using the above results. \n",
    "You should have found that the sign of $\\alpha$ can be either positive or negative. Choose $\\alpha$ to have the sign opposite to the one of $x^L_k[0]$. (Use the `numpy.sign`function).\n",
    "\n",
    "Try your implementation on the $B$ matrix seen in ex. 1. What is the advantage of this implementation ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc13bdb8-5046-42e4-a3e0-f58654acce81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qr_decomp_hh(A):\n",
    "    n = A.shape[0]\n",
    "    # your code here\n",
    "    return Q,R "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffc670d-b80e-4ad0-bf4a-54ec5bc9aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qh, rh = qr_decomp_hh(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514d869f-f2c7-40eb-b204-d593b6ee6133",
   "metadata": {},
   "source": [
    "*Answer:*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c34f30e-aa21-4bf0-8192-f7d43fa32e16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
